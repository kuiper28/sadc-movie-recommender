{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN_CF.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Qr3_cRCxdWi",
        "outputId": "8336f42c-c806-49b6-d879-d9ba2cb57d3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN_SRC = 128\n",
        "MAX_LEN_TAR = 0"
      ],
      "metadata": {
        "id": "KxzBdFMN93zJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJoKbhxU3o0S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a01e7b6c-6010-4a28-8714-3759c014c77a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max reviews by an user 1564\n"
          ]
        }
      ],
      "source": [
        "import operator\n",
        "import itertools\n",
        "from pprint import pprint\n",
        "\n",
        "class DataSequence():\n",
        "\n",
        "    def __init__(self, dataset, filename):\n",
        "        self.dataset = dataset\n",
        "        self.filename = filename\n",
        "    def getUserSequenes(self):\n",
        "        train_sequences = dict()\n",
        "        users = []\n",
        "        Items = set()\n",
        "        history = dict()\n",
        "        user = 0\n",
        "        nr_movies = 0\n",
        "        max_len = 0\n",
        "\n",
        "        watched = {}\n",
        "        with open(self.filename, \"r\") as f:\n",
        "            for line in f:\n",
        "                user, item, rating, timestamp = line.strip().split(\"::\")\n",
        "                user = int(user)\n",
        "                if ( int(float(rating)) >= 3):\n",
        "                  if int(item) > nr_movies:\n",
        "                    nr_movies = int(item)\n",
        "                  if user not in watched:\n",
        "                    watched[user] = []\n",
        "                  \n",
        "                  watched[user].append((int(item), timestamp))\n",
        "\n",
        "        for key in watched:\n",
        "          watched[key].sort(key = lambda x: x[1]) \n",
        "          if len(watched[key]) > max_len:\n",
        "            max_len = len(watched[key])\n",
        "        print(\"Max reviews by an user \" + str(max_len))\n",
        "\n",
        "        data_X = []\n",
        "        data_Y = []\n",
        "        for key in watched:\n",
        "          if len(watched[key]) > MAX_LEN_SRC+MAX_LEN_TAR:\n",
        "            latest_data = [x for (x,y) in watched[key]][-(MAX_LEN_SRC+MAX_LEN_TAR+1):]\n",
        "          else:\n",
        "            latest_data = [x for (x,y) in watched[key]]\n",
        "\n",
        "\n",
        "          append_X = latest_data[:-1]\n",
        "          append_Y = latest_data[1:]\n",
        "\n",
        "          append_X += [0] * (MAX_LEN_SRC+MAX_LEN_TAR - len(append_X)) \n",
        "          append_Y += [0] * (MAX_LEN_SRC+MAX_LEN_TAR - len(append_Y))\n",
        "\n",
        "          data_X.append(append_X)\n",
        "          data_Y.append(append_Y)\n",
        "\n",
        "          \n",
        "        \n",
        "\n",
        "        return data_X, data_Y, nr_movies\n",
        "\n",
        "data_sequence = DataSequence(\"MovieLens\", \"/content/drive/MyDrive/sadc/train.dat\")\n",
        "data_X, data_Y, nr_movies = data_sequence.getUserSequenes()\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "trainX, testX, trainY, testY = train_test_split(data_X, data_Y, test_size=0.1, random_state=42)\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(trainX)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aH1MBSlnDkkd",
        "outputId": "c4857881-9238-48b6-904f-a8f41da5be4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5435"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for x in data_Y:\n",
        "  if(len(x) != MAX_LEN_SRC):\n",
        "    print(\"error\")"
      ],
      "metadata": {
        "id": "pRL9hur7-ol_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import string\n",
        "import re\n",
        "from unicodedata import normalize\n",
        "import numpy as np\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from keras.models import Sequential,load_model\n",
        "from keras.layers import LSTM,Dense,Embedding,RepeatVector,TimeDistributed,Activation,Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from tensorflow.keras import activations\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from string import punctuation\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Markdown, display\n",
        "def printmd(string):\n",
        "    # Print with Markdowns    \n",
        "    display(Markdown(string))"
      ],
      "metadata": {
        "id": "OCBIf-0jDJuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nr_movies"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAjSAo4e87Gx",
        "outputId": "25715e10-10e7-4310-d132-08861237426d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3952"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab_size = nr_movies + 1\n",
        "tar_vocab_size = nr_movies + 1\n",
        "src_length = MAX_LEN_SRC \n",
        "tar_length = MAX_LEN_SRC\n",
        "\n",
        "print(f'\\nTarget Vocabulary Size: {tar_vocab_size}')\n",
        "print(f'Target Max Length: {tar_length}')\n",
        "\n",
        "print(f'\\nSource Vocabulary Size: {src_vocab_size}')\n",
        "print(f'Source Max Length: {src_length}\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OttQT5W2CNlw",
        "outputId": "bdf0d049-cc01-406c-94e7-a73e7a2f4a9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Target Vocabulary Size: 3953\n",
            "Target Max Length: 128\n",
            "\n",
            "Source Vocabulary Size: 3953\n",
            "Source Max Length: 128\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_output(sequences, vocab_size):\n",
        "    # one hot encode target sequence\n",
        "    ylist = list()\n",
        "    for sequence in sequences:\n",
        "        encoded = to_categorical(sequence, num_classes=vocab_size)\n",
        "        ylist.append(encoded)\n",
        "    y = np.array(ylist)\n",
        "    y = y.reshape(sequences.shape[0], sequences.shape[1], vocab_size)\n",
        "    return y"
      ],
      "metadata": {
        "id": "izn98MsnGp2q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def evaluate(test_model):\n",
        "  total_right10 = 0\n",
        "  total_right5 = 0\n",
        "  total_right3 = 0\n",
        "  total_right1 = 0\n",
        "  total = 0 \n",
        "  correct_movies = []\n",
        "  total_sps10 = 0\n",
        "  modified_sps = 0\n",
        "\n",
        "  for idx, x in enumerate(testX[:100]):\n",
        "    half1, half2 = getHalves(x)\n",
        "    count = np.count_nonzero(half1)\n",
        "    user_predictions = []\n",
        "\n",
        "    source = np.array(half1)\n",
        "    source = source.reshape((1, source.shape[0]))\n",
        "    prediction = test_model.predict(source, verbose=0)[0]\n",
        "    integers2 = [np.argmax(vector) for vector in prediction]\n",
        "    user_predictions10 = np.argpartition(prediction[count-1], -10)[-10:]\n",
        "    user_predictions5 = np.argpartition(prediction[count-1], -5)[-5:]\n",
        "    user_predictions3 = np.argpartition(prediction[count-1], -3)[-3:]\n",
        "    user_predictions1 = np.argpartition(prediction[count-1], -1)[-1:]\n",
        "    \n",
        "    total += 1\n",
        "    if half2[0] in user_predictions10:\n",
        "      total_sps10 += 1\n",
        "    for x in half2[:3]:\n",
        "      if x in user_predictions10:\n",
        "        modified_sps += 1\n",
        "        break;\n",
        "    \n",
        "    for pred in user_predictions10:\n",
        "      if pred in half2:\n",
        "        correct_movies.append(pred)\n",
        "        total_right10 += 1\n",
        "        break\n",
        "    # for pred in user_predictions5:\n",
        "    #   if pred in half2:\n",
        "    #     total_right5 += 1\n",
        "    #     break\n",
        "    # for pred in user_predictions3:\n",
        "    #   if pred in half2:\n",
        "    #     total_right3 += 1\n",
        "    #     break\n",
        "    # for pred in user_predictions1:\n",
        "    #   if pred in half2:\n",
        "    #     total_right1 += 1\n",
        "    #     break\n",
        "    \n",
        "  print(\"Item Coverage: \", len(Counter(correct_movies).keys()))\n",
        "  print(\"User Coverage: \", total_right10/total)\n",
        "  print(\"SPS1@10: \", total_sps10/total)\n",
        "  print(\"SPS3@10: \", modified_sps/total)\n",
        "  \n",
        "  return total_sps10/total, len(Counter(correct_movies).keys())\n",
        "  # print(\"\", total_right5/total)\n",
        "  # print(\"\", total_right3/total)\n",
        "  # print(\"\", total_right1/total)"
      ],
      "metadata": {
        "id": "x5C-ouGFKiBG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhcPXwFvw3PZ",
        "outputId": "cb95e9f7-4879-4730-e35a-90681f414fc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def getHalves(x):\n",
        "  length = np.count_nonzero(x)\n",
        "  half1 = x[:length//2]\n",
        "  half2 = x[:length][-length//2:]\n",
        "\n",
        "  # print(len(half1))\n",
        "  # print(len(half2))\n",
        "\n",
        "  half1 += [0] * (MAX_LEN_SRC+MAX_LEN_TAR - len(half1))\n",
        "  # print(len(half1))\n",
        "  return half1, half2"
      ],
      "metadata": {
        "id": "nqvJStHSREyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "total = 0\n",
        "for x in trainX:\n",
        "  keys = Counter(x).keys() # equals to list(set(words))\n",
        "  values = Counter(x).values() # counts the elements' frequency\n",
        "  for key, value in zip(keys, values):\n",
        "    if value > 1 and key != 0:\n",
        "      print(value, key)\n",
        "      break\n",
        "\n",
        "print(total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K98jpDbDOGpT",
        "outputId": "b02a2504-ee6e-47bc-b231-cc71dba2b0cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "def create_model(src_vocab, tar_vocab, src_timesteps, tar_timesteps, n_units):\n",
        "    # Create the model\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(src_vocab, n_units, input_length=src_timesteps, mask_zero=True))\n",
        "    model.add(LSTM(120, return_sequences=True))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(tar_vocab, activation='softmax'))\n",
        "    return model\n",
        " \n",
        "# Create model\n",
        "model = create_model(src_vocab_size, tar_vocab_size, src_length, tar_length, 64)\n",
        "# opt = tf.optimizers.Adagrad(lr=1, epsilon=1e-3)\n",
        "opt = tf.optimizers.Adam(lr=0.005)\n",
        "model.compile(optimizer=opt, loss=\"categorical_crossentropy\")\n",
        "\n",
        "from random import shuffle\n",
        "import time\n",
        "\n",
        "def get_batch(x, y, batch_size):\n",
        "  c = list(zip(x, y))\n",
        "\n",
        "  shuffle(c)\n",
        "\n",
        "  x, y = zip(*c)\n",
        "\n",
        "  return np.array(x[:batch_size]), encode_output(np.array(y[:batch_size]), tar_vocab_size)\n",
        "\n",
        "test_X, test_Y = get_batch(testX, testY, 128)\n",
        "\n",
        "\n",
        "val_loss =[]\n",
        "train_loss = []\n",
        "min_val_loss = 1000\n",
        "max_sps = 0\n",
        "max_item = 0\n",
        "saved_model = None\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "\n",
        "for epoch in range(0,100):\n",
        "  epoch_start_time = time.time()\n",
        "\n",
        "  train_res = 0\n",
        "  batch_size = 32\n",
        "  batches = int(len(trainX) /batch_size)\n",
        "  for batch in range(0,batches):\n",
        "    train_X, train_Y = get_batch(trainX, trainY, batch_size)\n",
        "    train_res += model.train_on_batch(\n",
        "      train_X, train_Y\n",
        "    )\n",
        "\n",
        "  test_res = model.test_on_batch(\n",
        "    test_X, test_Y\n",
        "  )\n",
        "\n",
        "  sps, item = evaluate(model)\n",
        "  if sps > max_sps:\n",
        "    max_sps = sps\n",
        "    model.save(\"/content/drive/MyDrive/sadc/testmodels3/max_sps\")\n",
        "  \n",
        "  if item > max_item:\n",
        "    max_item = item\n",
        "    model.save(\"/content/drive/MyDrive/sadc/testmodels3/max_item\")\n",
        "\n",
        "  val_loss.append(test_res)\n",
        "  train_loss.append(train_res/batches)\n",
        "  print(\"Epoch\", epoch,  int(time.time() - epoch_start_time), \"seconds\",\"train:\", train_res/batches, \"validation\", test_res , \"Total\", int(time.time() - start_time), \"seconds\")\n",
        "\n"
      ],
      "metadata": {
        "id": "cZY_qZufDb8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "items_model = tf.keras.models.load_model('/content/drive/MyDrive/sadc/testmodels2/max_item')\n",
        "sps_model = tf.keras.models.load_model('/content/drive/MyDrive/sadc/testmodels2/max_sps')"
      ],
      "metadata": {
        "id": "YWIK9YV6eV3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "sps_model.save_weights('/content/drive/MyDrive/sadc_model/checkpoint')\n"
      ],
      "metadata": {
        "id": "9yQey-AufnjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate(items_model)\n",
        "\n",
        "evaluate(sps_model)"
      ],
      "metadata": {
        "id": "F_yOCOsffFdO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}